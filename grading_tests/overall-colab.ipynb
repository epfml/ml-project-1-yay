{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-26T13:53:39.958806032Z",
          "start_time": "2023-10-26T13:53:38.828203084Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR5nz3uGhe_1",
        "outputId": "036c337a-6d1e-42c7-88a5-86b738cee5a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/grading_tests')\n",
        "from helpers import *\n",
        "from implementations import *"
      ],
      "metadata": {
        "id": "wbFgJ_kchoBg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wE8UeiCShe_8",
        "outputId": "67047762-b69f-4d2e-fd71-1022153c7878"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/gladys/Desktop/project1/grading_tests'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-10-26T13:53:39.955737596Z"
        },
        "is_executing": true,
        "id": "WMkmJFv8he_-"
      },
      "outputs": [],
      "source": [
        "# loading the data\n",
        "data_path = '/content/drive/MyDrive/Colab Notebooks/data/dataset_to_release'\n",
        "x_train_preclean, x_test_preclean, y_train, train_ids, test_ids = load_csv_data(data_path, sub_sample=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiG53St4he_-"
      },
      "source": [
        "# Getting an idea of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "is_executing": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LKy7RRdhfAB",
        "outputId": "3a972be9-1cbe-4cfb-f6da-a9c69838a2ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X train (6563, 321)\n",
            "X test (109379, 321)\n"
          ]
        }
      ],
      "source": [
        "print(\"X train\", x_train_preclean.shape)\n",
        "print(\"X test\", x_test_preclean.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "is_executing": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "t3coPBtQhfAI",
        "outputId": "af16f988-1435-477d-fee8-ff7210c47841"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, '# of columns')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "## Find how many values are completely empty in column\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def percentageFilled(data):\n",
        "    return 1 - np.isnan(data).sum() / len(data)\n",
        "\n",
        "percentage_filled = np.apply_along_axis(percentageFilled, 0, x_train_preclean)\n",
        "\n",
        "plt.hist(percentage_filled, bins=20)\n",
        "plt.title(\"Percentage of filled values per column\")\n",
        "plt.xlabel(\"Percentage\")\n",
        "plt.ylabel(\"# of columns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UEZXt4khfAI"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "is_executing": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "bHMHa_TwhfAJ"
      },
      "outputs": [],
      "source": [
        "## Process data\n",
        "## 1. drop the columns with more than 80% missing values\n",
        "def threshold_col_filter(data, threshold):\n",
        "    \"\"\"\n",
        "    filter out data where the column has less than threshold percentage of data\n",
        "    returns:\n",
        "        indicies of columns to keep\n",
        "    \"\"\"\n",
        "    percentage_filled = np.apply_along_axis(percentageFilled, 0, data)\n",
        "    # keep_indicies = np.argwhere(percentage_filled > threshold).flatten()\n",
        "    return percentage_filled > threshold\n",
        "\n",
        "\n",
        "def non_constant_filter(data):\n",
        "    \"\"\"\n",
        "    filter out where the values in the column are all the same\n",
        "    \"\"\"\n",
        "    return np.logical_not(np.logical_or(np.isnan(np.nanstd(data, 0)), np.nanstd(data, 0) == 0))\n",
        "\n",
        "# TODO uncorrelation?\n",
        "\n",
        "\n",
        "# TODO correlation w\n",
        "## SEE LATER, done at a later stage, after these two steps\n",
        "\n",
        "\n",
        "keep_indicies = np.argwhere(np.logical_and(\n",
        "    threshold_col_filter(x_train_preclean, 0.2),\n",
        "    non_constant_filter(x_train_preclean))\n",
        ").flatten()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "qTMg0Dv6hfAJ"
      },
      "outputs": [],
      "source": [
        "def standardize(x):\n",
        "    \"\"\"Standardize the original data set.\"\"\"\n",
        "    std = np.nanstd(x, axis=0)\n",
        "    mean = np.nanmean(x, axis=0)\n",
        "    return np.nan_to_num((x - np.nanmean(x, axis=0)) / np.nanstd(x, axis=0)), mean, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "-Wz8fQ3thfAJ"
      },
      "outputs": [],
      "source": [
        "def transform_train(feature):\n",
        "    m = dict()\n",
        "    for x in feature:\n",
        "        if x not in m:\n",
        "            m[x] = len(m)\n",
        "    f = np.vstack((np.eye(len(m)), np.zeros(len(m))))\n",
        "    u = f[np.vectorize(lambda key: m.get(key, len(m)))(feature)]\n",
        "    return u, m\n",
        "\n",
        "\n",
        "def transform_test(feature, m):\n",
        "    n_uniq = len(m)\n",
        "    f = np.vstack((np.eye(n_uniq), np.zeros(n_uniq)))\n",
        "    ind = np.array([m[k] if k in m else n_uniq for k in feature])\n",
        "    return f[ind]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "a8UHWoaQhfAK"
      },
      "outputs": [],
      "source": [
        "def process_train(data, corr_threshold = 0.5, cat_threshold = 10, verbose = False):\n",
        "    n, m = data.shape\n",
        "    filter = np.logical_and(threshold_col_filter(data, 0.2), non_constant_filter(data))\n",
        "    categorical_filter = np.apply_along_axis(lambda x: len(set(x)) < cat_threshold, 0, data)\n",
        "    cat_transform = dict()\n",
        "    num_transform = dict()\n",
        "    features = np.empty((n, 0))\n",
        "    transform_log = list()\n",
        "    cnt = 0\n",
        "    for i in range(m):\n",
        "        if not filter[i]:\n",
        "            transform_log.append(f\"{i} deleted\")\n",
        "            cnt += 1\n",
        "            continue\n",
        "        if categorical_filter[i]:\n",
        "            encoded, mp = transform_train(data[:, i])\n",
        "            l = encoded.shape[1]\n",
        "            transform_log.append(f\"{i} cat exp: {cnt}-{cnt + l - 1}\")\n",
        "            cnt += l\n",
        "            cat_transform[i] = mp\n",
        "            features = np.append(features, encoded, axis=1)\n",
        "        else:\n",
        "            transform_log.append(f\"{i} num: {cnt}\")\n",
        "            cnt += 1\n",
        "            x_num_std, mean, std = standardize(data[:, i])\n",
        "            x_num_std[abs(x_num_std) > 3] = 0\n",
        "            num_transform[i] = (mean, std)\n",
        "            features = np.append(features, x_num_std.reshape((n,1)), axis=1)\n",
        "    corr_filter = np.full((features.shape[1],), True, dtype=bool)\n",
        "    cm = np.corrcoef(features, rowvar=False)\n",
        "    for i in range(len(cm)):\n",
        "        if corr_filter[i]:\n",
        "            for j in range(i + 1, len(cm)):\n",
        "                if corr_filter[j] and abs(cm[i][j]) >= corr_threshold:\n",
        "                    corr_filter[j] = False\n",
        "    if verbose:\n",
        "        print(f\"number of features with pairwise corr <= {corr_threshold}: {sum(corr_filter)}\")\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n\".join(transform_log))\n",
        "    return features[:, corr_filter], filter, categorical_filter, corr_filter, num_transform, cat_transform\n",
        "\n",
        "\n",
        "def process_test(data, filter, categorical_filter, corr_filter, num_transform, cat_transform, verbose=False):\n",
        "    n, m = data.shape\n",
        "    res = np.empty((n, 0))\n",
        "    transform_log = list()\n",
        "    cnt = 0\n",
        "    for i in range(m):\n",
        "        if not filter[i]:\n",
        "            transform_log.append(f\"{i} deleted\")\n",
        "            cnt += 1\n",
        "            continue\n",
        "        if categorical_filter[i]:\n",
        "            l = len(cat_transform[i])\n",
        "            transform_log.append(f\"{i} cat exp: {cnt}-{cnt + l - 1}\")\n",
        "            cnt += l\n",
        "            res = np.append(res, transform_test(data[:, i], cat_transform[i]), axis=1)\n",
        "        else:\n",
        "            transform_log.append(f\"{i} num: {cnt}\")\n",
        "            cnt += 1\n",
        "            mean, std = num_transform[i] # std shouldn't be 0\n",
        "            res = np.append(res, np.nan_to_num((data[:, i] - mean) / std).reshape((n,1)), axis=1)\n",
        "    if verbose:\n",
        "        print(\"\\n\".join(transform_log))\n",
        "    return res[:, corr_filter]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgDY5h4WhfAK",
        "outputId": "0ad9d7db-34d6-44a3-a757-a4a32477c39b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of features with pairwise corr <= 0.5: 214\n",
            "0 num: 0\n",
            "1 num: 1\n",
            "2 num: 2\n",
            "3 num: 3\n",
            "4 num: 4\n",
            "5 cat exp: 5-6\n",
            "6 cat exp: 7-8\n",
            "7 num: 9\n",
            "8 num: 10\n",
            "9 deleted\n",
            "10 num: 12\n",
            "11 deleted\n",
            "12 deleted\n",
            "13 num: 15\n",
            "14 deleted\n",
            "15 num: 17\n",
            "16 num: 18\n",
            "17 num: 19\n",
            "18 deleted\n",
            "19 deleted\n",
            "20 num: 22\n",
            "21 num: 23\n",
            "22 deleted\n",
            "23 num: 25\n",
            "24 num: 26\n",
            "25 num: 27\n",
            "26 cat exp: 28-34\n",
            "27 num: 35\n",
            "28 num: 36\n",
            "29 num: 37\n",
            "30 cat exp: 38-41\n",
            "31 cat exp: 42-46\n",
            "32 cat exp: 47-50\n",
            "33 cat exp: 51-57\n",
            "34 cat exp: 58-63\n",
            "35 num: 64\n",
            "36 cat exp: 65-68\n",
            "37 num: 69\n",
            "38 num: 70\n",
            "39 cat exp: 71-73\n",
            "40 cat exp: 74-77\n",
            "41 deleted\n",
            "42 cat exp: 79-82\n",
            "43 cat exp: 83-86\n",
            "44 cat exp: 87-90\n",
            "45 cat exp: 91-94\n",
            "46 cat exp: 95-98\n",
            "47 cat exp: 99-102\n",
            "48 cat exp: 103-108\n",
            "49 deleted\n",
            "50 cat exp: 110-111\n",
            "51 cat exp: 112-118\n",
            "52 cat exp: 119-125\n",
            "53 cat exp: 126-130\n",
            "54 num: 131\n",
            "55 deleted\n",
            "56 num: 133\n",
            "57 cat exp: 134-137\n",
            "58 cat exp: 138-146\n",
            "59 num: 147\n",
            "60 num: 148\n",
            "61 num: 149\n",
            "62 num: 150\n",
            "63 num: 151\n",
            "64 deleted\n",
            "65 num: 153\n",
            "66 num: 154\n",
            "67 num: 155\n",
            "68 num: 156\n",
            "69 num: 157\n",
            "70 num: 158\n",
            "71 num: 159\n",
            "72 num: 160\n",
            "73 num: 161\n",
            "74 deleted\n",
            "75 num: 163\n",
            "76 num: 164\n",
            "77 num: 165\n",
            "78 num: 166\n",
            "79 num: 167\n",
            "80 num: 168\n",
            "81 num: 169\n",
            "82 num: 170\n",
            "83 num: 171\n",
            "84 num: 172\n",
            "85 num: 173\n",
            "86 num: 174\n",
            "87 num: 175\n",
            "88 num: 176\n",
            "89 num: 177\n",
            "90 num: 178\n",
            "91 num: 179\n",
            "92 num: 180\n",
            "93 num: 181\n",
            "94 num: 182\n",
            "95 num: 183\n",
            "96 num: 184\n",
            "97 num: 185\n",
            "98 num: 186\n",
            "99 num: 187\n",
            "100 num: 188\n",
            "101 num: 189\n",
            "102 num: 190\n",
            "103 num: 191\n",
            "104 num: 192\n",
            "105 num: 193\n",
            "106 num: 194\n",
            "107 deleted\n",
            "108 deleted\n",
            "109 deleted\n",
            "110 deleted\n",
            "111 deleted\n",
            "112 deleted\n",
            "113 deleted\n",
            "114 deleted\n",
            "115 deleted\n",
            "116 deleted\n",
            "117 deleted\n",
            "118 num: 206\n",
            "119 deleted\n",
            "120 deleted\n",
            "121 deleted\n",
            "122 deleted\n",
            "123 deleted\n",
            "124 deleted\n",
            "125 deleted\n",
            "126 deleted\n",
            "127 deleted\n",
            "128 deleted\n",
            "129 deleted\n",
            "130 deleted\n",
            "131 deleted\n",
            "132 deleted\n",
            "133 deleted\n",
            "134 deleted\n",
            "135 deleted\n",
            "136 num: 224\n",
            "137 deleted\n",
            "138 deleted\n",
            "139 deleted\n",
            "140 deleted\n",
            "141 deleted\n",
            "142 deleted\n",
            "143 deleted\n",
            "144 deleted\n",
            "145 deleted\n",
            "146 deleted\n",
            "147 deleted\n",
            "148 deleted\n",
            "149 deleted\n",
            "150 deleted\n",
            "151 deleted\n",
            "152 deleted\n",
            "153 deleted\n",
            "154 deleted\n",
            "155 deleted\n",
            "156 deleted\n",
            "157 deleted\n",
            "158 deleted\n",
            "159 deleted\n",
            "160 deleted\n",
            "161 deleted\n",
            "162 deleted\n",
            "163 deleted\n",
            "164 deleted\n",
            "165 deleted\n",
            "166 deleted\n",
            "167 deleted\n",
            "168 deleted\n",
            "169 deleted\n",
            "170 deleted\n",
            "171 deleted\n",
            "172 deleted\n",
            "173 deleted\n",
            "174 deleted\n",
            "175 deleted\n",
            "176 deleted\n",
            "177 deleted\n",
            "178 deleted\n",
            "179 deleted\n",
            "180 deleted\n",
            "181 deleted\n",
            "182 deleted\n",
            "183 deleted\n",
            "184 deleted\n",
            "185 deleted\n",
            "186 deleted\n",
            "187 deleted\n",
            "188 deleted\n",
            "189 deleted\n",
            "190 deleted\n",
            "191 deleted\n",
            "192 deleted\n",
            "193 deleted\n",
            "194 deleted\n",
            "195 deleted\n",
            "196 deleted\n",
            "197 deleted\n",
            "198 num: 286\n",
            "199 num: 287\n",
            "200 deleted\n",
            "201 deleted\n",
            "202 deleted\n",
            "203 deleted\n",
            "204 deleted\n",
            "205 deleted\n",
            "206 deleted\n",
            "207 deleted\n",
            "208 deleted\n",
            "209 deleted\n",
            "210 deleted\n",
            "211 deleted\n",
            "212 deleted\n",
            "213 deleted\n",
            "214 deleted\n",
            "215 deleted\n",
            "216 cat exp: 304-311\n",
            "217 cat exp: 312-314\n",
            "218 num: 315\n",
            "219 num: 316\n",
            "220 num: 317\n",
            "221 num: 318\n",
            "222 num: 319\n",
            "223 num: 320\n",
            "224 deleted\n",
            "225 deleted\n",
            "226 deleted\n",
            "227 cat exp: 324-326\n",
            "228 num: 327\n",
            "229 num: 328\n",
            "230 cat exp: 329-331\n",
            "231 cat exp: 332-334\n",
            "232 cat exp: 335-337\n",
            "233 cat exp: 338-341\n",
            "234 num: 342\n",
            "235 cat exp: 343-345\n",
            "236 cat exp: 346-348\n",
            "237 cat exp: 349-352\n",
            "238 num: 353\n",
            "239 cat exp: 354-362\n",
            "240 cat exp: 363-371\n",
            "241 cat exp: 372-374\n",
            "242 cat exp: 375-383\n",
            "243 cat exp: 384-386\n",
            "244 cat exp: 387-392\n",
            "245 num: 393\n",
            "246 num: 394\n",
            "247 cat exp: 395-397\n",
            "248 num: 398\n",
            "249 cat exp: 399-404\n",
            "250 num: 405\n",
            "251 num: 406\n",
            "252 num: 407\n",
            "253 num: 408\n",
            "254 num: 409\n",
            "255 cat exp: 410-412\n",
            "256 cat exp: 413-419\n",
            "257 cat exp: 420-424\n",
            "258 cat exp: 425-430\n",
            "259 cat exp: 431-435\n",
            "260 cat exp: 436-438\n",
            "261 cat exp: 439-442\n",
            "262 num: 443\n",
            "263 cat exp: 444-446\n",
            "264 num: 447\n",
            "265 cat exp: 448-450\n",
            "266 num: 451\n",
            "267 num: 452\n",
            "268 num: 453\n",
            "269 num: 454\n",
            "270 num: 455\n",
            "271 num: 456\n",
            "272 cat exp: 457-459\n",
            "273 cat exp: 460-464\n",
            "274 cat exp: 465-466\n",
            "275 cat exp: 467-468\n",
            "276 num: 469\n",
            "277 num: 470\n",
            "278 cat exp: 471-473\n",
            "279 cat exp: 474-476\n",
            "280 deleted\n",
            "281 deleted\n",
            "282 cat exp: 479-480\n",
            "283 cat exp: 481-482\n",
            "284 cat exp: 483-485\n",
            "285 num: 486\n",
            "286 num: 487\n",
            "287 num: 488\n",
            "288 num: 489\n",
            "289 num: 490\n",
            "290 num: 491\n",
            "291 num: 492\n",
            "292 num: 493\n",
            "293 num: 494\n",
            "294 num: 495\n",
            "295 num: 496\n",
            "296 num: 497\n",
            "297 num: 498\n",
            "298 cat exp: 499-501\n",
            "299 num: 502\n",
            "300 num: 503\n",
            "301 num: 504\n",
            "302 num: 505\n",
            "303 num: 506\n",
            "304 num: 507\n",
            "305 cat exp: 508-512\n",
            "306 cat exp: 513-515\n",
            "307 cat exp: 516-519\n",
            "308 cat exp: 520-523\n",
            "309 cat exp: 524-526\n",
            "310 cat exp: 527-529\n",
            "311 cat exp: 530-534\n",
            "312 cat exp: 535-537\n",
            "313 num: 538\n",
            "314 num: 539\n",
            "315 num: 540\n",
            "316 cat exp: 541-543\n",
            "317 cat exp: 544-546\n",
            "318 num: 547\n",
            "319 num: 548\n",
            "320 num: 549\n",
            "0 num: 0\n",
            "1 num: 1\n",
            "2 num: 2\n",
            "3 num: 3\n",
            "4 num: 4\n",
            "5 cat exp: 5-6\n",
            "6 cat exp: 7-8\n",
            "7 num: 9\n",
            "8 num: 10\n",
            "9 deleted\n",
            "10 num: 12\n",
            "11 deleted\n",
            "12 deleted\n",
            "13 num: 15\n",
            "14 deleted\n",
            "15 num: 17\n",
            "16 num: 18\n",
            "17 num: 19\n",
            "18 deleted\n",
            "19 deleted\n",
            "20 num: 22\n",
            "21 num: 23\n",
            "22 deleted\n",
            "23 num: 25\n",
            "24 num: 26\n",
            "25 num: 27\n",
            "26 cat exp: 28-34\n",
            "27 num: 35\n",
            "28 num: 36\n",
            "29 num: 37\n",
            "30 cat exp: 38-41\n",
            "31 cat exp: 42-46\n",
            "32 cat exp: 47-50\n",
            "33 cat exp: 51-57\n",
            "34 cat exp: 58-63\n",
            "35 num: 64\n",
            "36 cat exp: 65-68\n",
            "37 num: 69\n",
            "38 num: 70\n",
            "39 cat exp: 71-73\n",
            "40 cat exp: 74-77\n",
            "41 deleted\n",
            "42 cat exp: 79-82\n",
            "43 cat exp: 83-86\n",
            "44 cat exp: 87-90\n",
            "45 cat exp: 91-94\n",
            "46 cat exp: 95-98\n",
            "47 cat exp: 99-102\n",
            "48 cat exp: 103-108\n",
            "49 deleted\n",
            "50 cat exp: 110-111\n",
            "51 cat exp: 112-118\n",
            "52 cat exp: 119-125\n",
            "53 cat exp: 126-130\n",
            "54 num: 131\n",
            "55 deleted\n",
            "56 num: 133\n",
            "57 cat exp: 134-137\n",
            "58 cat exp: 138-146\n",
            "59 num: 147\n",
            "60 num: 148\n",
            "61 num: 149\n",
            "62 num: 150\n",
            "63 num: 151\n",
            "64 deleted\n",
            "65 num: 153\n",
            "66 num: 154\n",
            "67 num: 155\n",
            "68 num: 156\n",
            "69 num: 157\n",
            "70 num: 158\n",
            "71 num: 159\n",
            "72 num: 160\n",
            "73 num: 161\n",
            "74 deleted\n",
            "75 num: 163\n",
            "76 num: 164\n",
            "77 num: 165\n",
            "78 num: 166\n",
            "79 num: 167\n",
            "80 num: 168\n",
            "81 num: 169\n",
            "82 num: 170\n",
            "83 num: 171\n",
            "84 num: 172\n",
            "85 num: 173\n",
            "86 num: 174\n",
            "87 num: 175\n",
            "88 num: 176\n",
            "89 num: 177\n",
            "90 num: 178\n",
            "91 num: 179\n",
            "92 num: 180\n",
            "93 num: 181\n",
            "94 num: 182\n",
            "95 num: 183\n",
            "96 num: 184\n",
            "97 num: 185\n",
            "98 num: 186\n",
            "99 num: 187\n",
            "100 num: 188\n",
            "101 num: 189\n",
            "102 num: 190\n",
            "103 num: 191\n",
            "104 num: 192\n",
            "105 num: 193\n",
            "106 num: 194\n",
            "107 deleted\n",
            "108 deleted\n",
            "109 deleted\n",
            "110 deleted\n",
            "111 deleted\n",
            "112 deleted\n",
            "113 deleted\n",
            "114 deleted\n",
            "115 deleted\n",
            "116 deleted\n",
            "117 deleted\n",
            "118 num: 206\n",
            "119 deleted\n",
            "120 deleted\n",
            "121 deleted\n",
            "122 deleted\n",
            "123 deleted\n",
            "124 deleted\n",
            "125 deleted\n",
            "126 deleted\n",
            "127 deleted\n",
            "128 deleted\n",
            "129 deleted\n",
            "130 deleted\n",
            "131 deleted\n",
            "132 deleted\n",
            "133 deleted\n",
            "134 deleted\n",
            "135 deleted\n",
            "136 num: 224\n",
            "137 deleted\n",
            "138 deleted\n",
            "139 deleted\n",
            "140 deleted\n",
            "141 deleted\n",
            "142 deleted\n",
            "143 deleted\n",
            "144 deleted\n",
            "145 deleted\n",
            "146 deleted\n",
            "147 deleted\n",
            "148 deleted\n",
            "149 deleted\n",
            "150 deleted\n",
            "151 deleted\n",
            "152 deleted\n",
            "153 deleted\n",
            "154 deleted\n",
            "155 deleted\n",
            "156 deleted\n",
            "157 deleted\n",
            "158 deleted\n",
            "159 deleted\n",
            "160 deleted\n",
            "161 deleted\n",
            "162 deleted\n",
            "163 deleted\n",
            "164 deleted\n",
            "165 deleted\n",
            "166 deleted\n",
            "167 deleted\n",
            "168 deleted\n",
            "169 deleted\n",
            "170 deleted\n",
            "171 deleted\n",
            "172 deleted\n",
            "173 deleted\n",
            "174 deleted\n",
            "175 deleted\n",
            "176 deleted\n",
            "177 deleted\n",
            "178 deleted\n",
            "179 deleted\n",
            "180 deleted\n",
            "181 deleted\n",
            "182 deleted\n",
            "183 deleted\n",
            "184 deleted\n",
            "185 deleted\n",
            "186 deleted\n",
            "187 deleted\n",
            "188 deleted\n",
            "189 deleted\n",
            "190 deleted\n",
            "191 deleted\n",
            "192 deleted\n",
            "193 deleted\n",
            "194 deleted\n",
            "195 deleted\n",
            "196 deleted\n",
            "197 deleted\n",
            "198 num: 286\n",
            "199 num: 287\n",
            "200 deleted\n",
            "201 deleted\n",
            "202 deleted\n",
            "203 deleted\n",
            "204 deleted\n",
            "205 deleted\n",
            "206 deleted\n",
            "207 deleted\n",
            "208 deleted\n",
            "209 deleted\n",
            "210 deleted\n",
            "211 deleted\n",
            "212 deleted\n",
            "213 deleted\n",
            "214 deleted\n",
            "215 deleted\n",
            "216 cat exp: 304-311\n",
            "217 cat exp: 312-314\n",
            "218 num: 315\n",
            "219 num: 316\n",
            "220 num: 317\n",
            "221 num: 318\n",
            "222 num: 319\n",
            "223 num: 320\n",
            "224 deleted\n",
            "225 deleted\n",
            "226 deleted\n",
            "227 cat exp: 324-326\n",
            "228 num: 327\n",
            "229 num: 328\n",
            "230 cat exp: 329-331\n",
            "231 cat exp: 332-334\n",
            "232 cat exp: 335-337\n",
            "233 cat exp: 338-341\n",
            "234 num: 342\n",
            "235 cat exp: 343-345\n",
            "236 cat exp: 346-348\n",
            "237 cat exp: 349-352\n",
            "238 num: 353\n",
            "239 cat exp: 354-362\n",
            "240 cat exp: 363-371\n",
            "241 cat exp: 372-374\n",
            "242 cat exp: 375-383\n",
            "243 cat exp: 384-386\n",
            "244 cat exp: 387-392\n",
            "245 num: 393\n",
            "246 num: 394\n",
            "247 cat exp: 395-397\n",
            "248 num: 398\n",
            "249 cat exp: 399-404\n",
            "250 num: 405\n",
            "251 num: 406\n",
            "252 num: 407\n",
            "253 num: 408\n",
            "254 num: 409\n",
            "255 cat exp: 410-412\n",
            "256 cat exp: 413-419\n",
            "257 cat exp: 420-424\n",
            "258 cat exp: 425-430\n",
            "259 cat exp: 431-435\n",
            "260 cat exp: 436-438\n",
            "261 cat exp: 439-442\n",
            "262 num: 443\n",
            "263 cat exp: 444-446\n",
            "264 num: 447\n",
            "265 cat exp: 448-450\n",
            "266 num: 451\n",
            "267 num: 452\n",
            "268 num: 453\n",
            "269 num: 454\n",
            "270 num: 455\n",
            "271 num: 456\n",
            "272 cat exp: 457-459\n",
            "273 cat exp: 460-464\n",
            "274 cat exp: 465-466\n",
            "275 cat exp: 467-468\n",
            "276 num: 469\n",
            "277 num: 470\n",
            "278 cat exp: 471-473\n",
            "279 cat exp: 474-476\n",
            "280 deleted\n",
            "281 deleted\n",
            "282 cat exp: 479-480\n",
            "283 cat exp: 481-482\n",
            "284 cat exp: 483-485\n",
            "285 num: 486\n",
            "286 num: 487\n",
            "287 num: 488\n",
            "288 num: 489\n",
            "289 num: 490\n",
            "290 num: 491\n",
            "291 num: 492\n",
            "292 num: 493\n",
            "293 num: 494\n",
            "294 num: 495\n",
            "295 num: 496\n",
            "296 num: 497\n",
            "297 num: 498\n",
            "298 cat exp: 499-501\n",
            "299 num: 502\n",
            "300 num: 503\n",
            "301 num: 504\n",
            "302 num: 505\n",
            "303 num: 506\n",
            "304 num: 507\n",
            "305 cat exp: 508-512\n",
            "306 cat exp: 513-515\n",
            "307 cat exp: 516-519\n",
            "308 cat exp: 520-523\n",
            "309 cat exp: 524-526\n",
            "310 cat exp: 527-529\n",
            "311 cat exp: 530-534\n",
            "312 cat exp: 535-537\n",
            "313 num: 538\n",
            "314 num: 539\n",
            "315 num: 540\n",
            "316 cat exp: 541-543\n",
            "317 cat exp: 544-546\n",
            "318 num: 547\n",
            "319 num: 548\n",
            "320 num: 549\n"
          ]
        }
      ],
      "source": [
        " x_train, filter, categorical_filter, corr_filter, num_transform, cat_transform = process_train(x_train_preclean, verbose=True)\n",
        "\n",
        " x_test = process_test(x_test_preclean, filter, categorical_filter, corr_filter, num_transform, cat_transform, verbose=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9GtaZiWhfAL",
        "outputId": "0a384fc0-73bc-4d9d-acfb-e4d9e0ab270f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# x_train shape: (6563, 214)\n"
          ]
        }
      ],
      "source": [
        "print(f\"# x_train shape: {x_train.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4qPiQhmhfAN"
      },
      "source": [
        "# Logistic regression *without* regularization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from lab2, put into helpers.py file\n",
        "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
        "    \"\"\"\n",
        "    Generate a minibatch iterator for a dataset.\n",
        "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
        "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
        "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
        "\n",
        "    Example:\n",
        "\n",
        "     Number of batches = 9\n",
        "\n",
        "     Batch size = 7                              Remainder = 3\n",
        "     v     v                                         v v\n",
        "    |-------|-------|-------|-------|-------|-------|---|\n",
        "        0       7       14      21      28      35   max batches = 6\n",
        "\n",
        "    If shuffle is False, the returned batches are the ones started from the indexes:\n",
        "    0, 7, 14, 21, 28, 35, 0, 7, 14\n",
        "\n",
        "    If shuffle is True, the returned batches start in:\n",
        "    7, 28, 14, 35, 14, 0, 21, 28, 7\n",
        "\n",
        "    To prevent the remainder datapoints from ever being taken into account, each of the shuffled indexes is added a random amount\n",
        "    8, 28, 16, 38, 14, 0, 22, 28, 9\n",
        "\n",
        "    This way batches might overlap, but the returned batches are slightly more representative.\n",
        "\n",
        "    Disclaimer: To keep this function simple, individual datapoints are not shuffled. For a more random result consider using a batch_size of 1.\n",
        "\n",
        "    Example of use :\n",
        "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
        "        <DO-SOMETHING>\n",
        "    \"\"\"\n",
        "    data_size = len(y)  # NUmber of data points.\n",
        "    batch_size = min(data_size, batch_size)  # Limit the possible size of the batch.\n",
        "    max_batches = int(\n",
        "        data_size / batch_size\n",
        "    )  # The maximum amount of non-overlapping batches that can be extracted from the data.\n",
        "    remainder = (\n",
        "        data_size - max_batches * batch_size\n",
        "    )  # Points that would be excluded if no overlap is allowed.\n",
        "\n",
        "    if shuffle:\n",
        "        # Generate an array of indexes indicating the start of each batch\n",
        "        idxs = np.random.randint(max_batches, size=num_batches) * batch_size\n",
        "        if remainder != 0:\n",
        "            # Add an random offset to the start of each batch to eventually consider the remainder points\n",
        "            idxs += np.random.randint(remainder + 1, size=num_batches)\n",
        "    else:\n",
        "        # If no shuffle is done, the array of indexes is circular.\n",
        "        idxs = np.array([i % max_batches for i in range(num_batches)]) * batch_size\n",
        "\n",
        "    for start in idxs:\n",
        "        start_index = start  # The first data point of the batch\n",
        "        end_index = (\n",
        "            start_index + batch_size\n",
        "        )  # The first data point of the following batch\n",
        "        yield y[start_index:end_index], tx[start_index:end_index]\n",
        "\n"
      ],
      "metadata": {
        "id": "LWt2hytKkye9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def logistic_gradient(y, tx, w):\n",
        "    return  tx.T.dot(sigmoid(tx.dot(w)) - y)/ len(y)\n",
        "def compute_gradient_logistic_loss_regularized(y, tx, w, lambda_):\n",
        "    \"\"\"Compute the gradient of the regularized logistic regression \"\"\"\n",
        "    grad = logistic_gradient(y, tx, w) + lambda_ * w\n",
        "    return grad\n",
        "\n",
        "def regularized_log_reg_sgd(y, tx, initial_w, max_iters, gamma,  lambda_ ):\n",
        "    \"\"\"Regularized logistic regression using stochastic gradient descent.\"\"\"\n",
        "    w = initial_w\n",
        "    prev_loss = float('inf')\n",
        "\n",
        "    for n_iter in range(max_iters):\n",
        "\t\t# Each iteration corresponds to one epoch (num_batches=len(y)) and each batch has size 1\n",
        "        for batch_y, batch_x in batch_iter(y, tx, 1, num_batches=len(y)):\n",
        "\t\t\t# Computing the gradient of the logistic loss with respect to w\n",
        "            gradient = compute_gradient_logistic_loss_regularized(batch_y, batch_x, w, lambda_)\n",
        "\t\t\t# Updating w\n",
        "            w -= gamma * gradient\n",
        "\n",
        "\n",
        "        loss = logistic_loss(y, tx, w) + (lambda_ / 2) * np.squeeze(w.T @ w)\n",
        "        if prev_loss <= loss:\n",
        "            gamma *= 0.1  # adapt step size\n",
        "        prev_loss = loss\n",
        "\n",
        "    return w, loss\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mS7Vzs8wk25H"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_k_indices(y, k_fold, seed):\n",
        "    \"\"\"build k indices for k-fold.\n",
        "\n",
        "    Args:\n",
        "        y:      shape=(N,)\n",
        "        k_fold: K in K-fold, i.e. the fold num\n",
        "        seed:   the random seed\n",
        "\n",
        "    Returns:\n",
        "        A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
        "\n",
        "    >>> build_k_indices(np.array([1., 2., 3., 4.]), 2, 1)\n",
        "    array([[3, 2],\n",
        "           [0, 1]])\n",
        "    \"\"\"\n",
        "    num_row = y.shape[0]\n",
        "    interval = int(num_row / k_fold)\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    indices = np.random.permutation(num_row)\n",
        "    k_indices = [indices[k * interval : (k + 1) * interval] for k in range(k_fold)]\n",
        "    return np.array(k_indices)"
      ],
      "metadata": {
        "id": "cUnmrCF8kX7y"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validation(y, x, k_fold, lambda_, seed):\n",
        "    \"\"\"\n",
        "    Separate the training set into k_fold parts, get k_fold sets of w weights, and return the average w\n",
        "    Args:\n",
        "        y,\n",
        "        x,\n",
        "        k_fold,\n",
        "        lambda_,\n",
        "\n",
        "    Return:\n",
        "        average weight\n",
        "        average losses\n",
        "    \"\"\"\n",
        "    k_indices = build_k_indices(y, k_fold, seed)\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    weights = []\n",
        "    for k in range(k_fold):\n",
        "        test_indices = k_indices[k]\n",
        "        train_indices = k_indices[~(np.arange(k_indices.shape[0]) == k)].flatten()\n",
        "        x_train = x[train_indices]\n",
        "        y_train = y[train_indices]\n",
        "        x_test = x[test_indices]\n",
        "        y_test = y[test_indices]\n",
        "\n",
        "        w, loss = regularized_log_reg_sgd(y_train, x_train, initial_w= np.zeros(x_train.shape[1]),max_iters=100, gamma=0.1, lambda_ =lambda_)\n",
        "        train_losses.append(loss)\n",
        "        test_loss = logistic_loss(y_test, x_test, w)\n",
        "        test_losses.append(test_loss)\n",
        "        weights.append(w)\n",
        "\n",
        "    return np.mean(weights, axis=0), np.mean(train_losses), np.mean(test_losses)"
      ],
      "metadata": {
        "id": "k9N_msp0kbxY"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(t):\n",
        "    \"\"\"\n",
        "      Calculate the sigmoid function for a given input.\n",
        "      Parameters:\n",
        "      t (float): The input value\n",
        "\n",
        "      Returns:\n",
        "      float: The result of the sigmoid function\n",
        "      \"\"\"\n",
        "    # return np.where(t < 0, np.exp(t)/(1.0 +np.exp(t)) , 1.0 / (1.0 + np.exp(-t)))  ##\n",
        "    return 1.0 / (1 + np.exp(-t))"
      ],
      "metadata": {
        "id": "zs4h4Ht1s3cb"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction_labels(weights, data):\n",
        "    \"\"\"Generates class predictions given weights, and a test data matrix.\"\"\"\n",
        "    y_pred = sigmoid(np.dot(data, weights))\n",
        "    # display(y_pred)\n",
        "    y_pred[np.where(y_pred >= 0.5)] = 1\n",
        "    y_pred[np.where(y_pred < 0.5)] = 0\n",
        "    return y_pred\n",
        "\n",
        "def accuracy(y_pred, y_train):\n",
        "    return (y_pred == y_train).sum() / len(y_train)\n",
        "def precision(y_pred, y_train):\n",
        "    TP = np.sum((y_train==1) & (y_pred==1))\n",
        "    FP = np.sum((y_train==0) & (y_pred==1))\n",
        "    return TP/(TP+FP)\n",
        "def recall(y_pred, y_train):\n",
        "    recall = np.sum((y_train==1) & (y_pred==1)) / np.sum(y_train==1)\n",
        "    return recall\n",
        "def f1_score (y_pred, y_train):\n",
        "    return 2*precision(y_pred, y_train)*recall(y_pred, y_train) / (precision(y_pred, y_train) + recall(y_pred, y_train))"
      ],
      "metadata": {
        "id": "YQG-GBhzlDN8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_parameters(y, tx, intitial_w, max_iters, k_fold, gamma, lambdas):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        y:\n",
        "        tx:\n",
        "        intitial_w:\n",
        "        max_iters:\n",
        "        gamma:\n",
        "        lambdas:\n",
        "        k_fold:\n",
        "\n",
        "    Returns: loss_tr, f1_scores\n",
        "\n",
        "    \"\"\"\n",
        "    seed = 55\n",
        "\n",
        "    weights = []\n",
        "    loss_tr = []\n",
        "    loss_tt = []\n",
        "    f1_scores = []\n",
        "    y_preds = []\n",
        "\n",
        "    for lambda_ in lambdas:\n",
        "        # make cross validation return weight\n",
        "        avg_w, avg_train_loss, avg_test_loss = cross_validation(y, tx, k_fold, lambda_, seed)\n",
        "        loss_tr.append(avg_train_loss)\n",
        "        loss_tt.append(avg_test_loss)\n",
        "        weights.append(avg_w)\n",
        "        y_pred = prediction_labels(avg_w, tx)\n",
        "        y_preds.append(y_pred)\n",
        "        f1_scores.append(f1_score(y_pred, y))\n",
        "\n",
        "\n",
        "    return loss_tr, loss_tt, f1_scores, weights, y_preds"
      ],
      "metadata": {
        "id": "YGGGJ0cQkf0U"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_tr,loss_tt, f1_scores, weights, y_preds = get_best_parameters(y_train, x_train, np.ones( x_train.shape[1]), 20, 5, 0.5, np.arange(0, 1, 0.1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7f2bgQ6k54G",
        "outputId": "85c28fa5-6134-47af-e803-41d1a2b8213a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-bf376d7a08a6>:14: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  return TP/(TP+FP)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_tr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGdnEtRzsWvI",
        "outputId": "728bf3b8-b9c5-40cb-fef7-d39627684899"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.26608793911039175, 0.296436724939687]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_tt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMpLbogG15Sg",
        "outputId": "a5e52062-cf5e-4416-b432-316075ddb036"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.24160823167059003, 0.25854117934376664]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f1_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3spsXWDywtiN",
        "outputId": "0bc1f1c6-88aa-4997-fa65-b3aef2f93d9d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0035460992907801418, nan]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_preds[0].sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkDExvsJwvtz",
        "outputId": "8ff5f7f5-5b82-486c-8b4a-ae50e6051cac"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}