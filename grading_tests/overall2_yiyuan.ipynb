{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:53:39.958806032Z",
     "start_time": "2023-10-26T13:53:38.828203084Z"
    }
   },
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from implementations import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-26T13:53:39.955737596Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "data_path = '../data/dataset_to_release'\n",
    "x_train_preclean, x_test_preclean, y_train, train_ids, test_ids = load_csv_data(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting an idea of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "print(\"X train\", x_train_preclean.shape)\n",
    "print(\"X test\", x_test_preclean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Find how many values are completely empty in column\n",
    "def percentageFilled(data):\n",
    "    return 1 - np.isnan(data).sum() / len(data)\n",
    "\n",
    "percentage_filled = np.apply_along_axis(percentageFilled, 0, x_train_preclean)\n",
    "# plt.hist(percentage_filled, bins=20)\n",
    "# plt.title(\"Percentage of filled values per column\")\n",
    "# plt.xlabel(\"Percentage\")\n",
    "# plt.ylabel(\"# of columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train (328135, 201)\n",
      "X test (109379, 201)\n"
     ]
    }
   ],
   "source": [
    "## Process data \n",
    "## 1. drop the columns with more than 80% missing values\n",
    "def threshold_col_filter(data, threshold):\n",
    "    \"\"\" \n",
    "    filter out data where the column has less than threshold percentage of data\n",
    "    returns: \n",
    "        indicies of columns to keep\n",
    "    \"\"\"\n",
    "    percentage_filled = np.apply_along_axis(percentageFilled, 0, data)\n",
    "    # keep_indicies = np.argwhere(percentage_filled > threshold).flatten()\n",
    "    return percentage_filled > threshold\n",
    "\n",
    "\n",
    "def non_constant_filter(data):\n",
    "    \"\"\"\n",
    "    filter out where the values in the column are all the same\n",
    "    \"\"\"\n",
    "    return np.logical_not(np.logical_or(np.isnan(np.nanstd(data, 0)), np.nanstd(data, 0) == 0))\n",
    "\n",
    "def filter_columns_by_indicies(data, keep_indicies):\n",
    "    \"\"\"\n",
    "    used to process test data \n",
    "    only keep the columns that are in the indicies \n",
    "    \"\"\"\n",
    "    return data[:, keep_indicies]\n",
    "\n",
    "keep_indicies = np.argwhere(np.logical_and(\n",
    "    threshold_col_filter(x_train_preclean, 0.2), \n",
    "    non_constant_filter(x_train_preclean))\n",
    ").flatten()\n",
    "\n",
    "x_train = filter_columns_by_indicies(x_train_preclean, keep_indicies)\n",
    "print(\"X train\", x_train.shape)\n",
    "\n",
    "x_test = filter_columns_by_indicies(x_test_preclean, keep_indicies)\n",
    "print(\"X test\", x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_categorical_feature(xn, threshold=10):\n",
    "    return len(set(xn)) < threshold\n",
    "\n",
    "# TODO: return two lists, indices of numerical and categorical features\n",
    "def split_num_cat(data):\n",
    "    is_cat_filter = np.apply_along_axis(is_categorical_feature, 0, data)\n",
    "    cat_idx = np.argwhere(is_cat_filter == True).flatten()\n",
    "    num_idx = np.argwhere(is_cat_filter == False).flatten()\n",
    "    return cat_idx, num_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_idx, num_idx = split_num_cat(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "x_train_cat = filter_columns_by_indicies(x_train, cat_idx)\n",
    "x_train_num = filter_columns_by_indicies(x_train, num_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## now let's do the one hot encoding\n",
    "def process_categorical(one_col):\n",
    "    \"\"\" \n",
    "    Change discontinuous categorical data to continuous categorical data\n",
    "    why do I need this: some categorical data has this format (1, 2, 7, 9), but encoding needs it to be (0, 1, 2, 3) continuous discrete numbers \n",
    "    \"\"\"\n",
    "    unique_values = np.sort(np.unique(one_col))\n",
    "    return_col = -1 * np.ones(one_col.shape)\n",
    "    for i in range(len(unique_values)):\n",
    "        return_col[one_col == unique_values[i]] = i \n",
    "    return return_col.astype(int)\n",
    "\n",
    "def encode_structure(x_train):\n",
    "    \"\"\" \n",
    "    feed it x_train categorical data, it returns the following list of list: \n",
    "    [[unique_values for col 1], [unique_values for col 2], [unique_values for col 3], ...]\n",
    "    \"\"\"\n",
    "    encode_structure = []\n",
    "    for i in range(x_train.shape[1]):\n",
    "        encode_structure.append(np.sort(np.unique(x_train[:,i])))\n",
    "    return encode_structure\n",
    "   \n",
    "def encoding_one_col(one_col):\n",
    "    \"\"\"\n",
    "    Explode one column of categorical into (unique values) columns of one hot encoding\n",
    "    \"\"\"\n",
    "    b = np.zeros((one_col.size, len(np.unique(one_col))))\n",
    "    b[np.arange(one_col.size), one_col] = 1\n",
    "    return b\n",
    "\n",
    "\n",
    "# TODO: returns the exploded data, and the unique values for each categorical feature\n",
    "def one_hot_encode_train(x_train): ## for training data exclusively \n",
    "    \"\"\"\n",
    "        Explode the categorical columns (x_cat array) into one hot encoding\n",
    "    \"\"\"\n",
    "    ## apply categorical \n",
    "    enum_x_train = np.apply_along_axis(process_categorical, 0, x_train)\n",
    "    encoded_x_train = np.empty((len(enum_x_train), 0))\n",
    "    for i in range(enum_x_train.shape[1]):\n",
    "        encoded_x_train = np.append(encoded_x_train, encoding_one_col(enum_x_train[:,i]), axis=1)\n",
    "    return encoded_x_train\n",
    "\n",
    "# TODO: explode based on the unique values of the training data\n",
    "def one_hot_encode_test(x_test, encode_structure):\n",
    "    \"\"\"\n",
    "        Explode the test data into one hot encoding based on encode_structure given \n",
    "    \"\"\"\n",
    "    def process_categorical_test(one_col, unique_values):\n",
    "        \"\"\"\n",
    "            unseen category is changed into -1 \n",
    "        \"\"\"\n",
    "        return_col = -1 * np.ones(one_col.shape)\n",
    "        for i in range(len(unique_values)):\n",
    "            return_col[one_col == unique_values[i]] = i \n",
    "        return return_col.astype(int)\n",
    "\n",
    "    def explode_one_column(one_col, unique_values):\n",
    "        \"\"\"\n",
    "            Explode one column of categorical into (unique values) columns of one hot encoding\n",
    "        \"\"\"\n",
    "        b = np.zeros((one_col.size, len(unique_values)))\n",
    "        for j in range(one_col.size): ## loop through vertically, row by row \n",
    "            if one_col[j] == -1: ## this line keeps the unseen data to zero \n",
    "                continue\n",
    "            else: \n",
    "                b[j, one_col[j]] = 1\n",
    "        return b\n",
    "   \n",
    "    ## apply the categorical value according to the encode_structure\n",
    "    for col_idx in range(x_test.shape[1]):\n",
    "        x_test[:, col_idx] = process_categorical_test(x_test[:, col_idx], encode_structure[col_idx])\n",
    "\n",
    "    ## explode it \n",
    "    encoded_x_test = np.empty((len(x_test), 0))\n",
    "    for i in range(x_test.shape[1]):\n",
    "        encoded_x_test = np.append(encoded_x_test, explode_one_column(x_test[:,i], encode_structure[i]), axis=1)\n",
    "    return encoded_x_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the outliers\n",
    "def remove_outliers(x_num, threshold = 3):\n",
    "    x = x_num.copy()\n",
    "    removed_count = 0\n",
    "    for col in range(x.shape[1]):\n",
    "        std = np.std(x[:, col])\n",
    "        range_ = [-threshold*std, threshold*std]\n",
    "        # Count outliers for this column\n",
    "        removed_count_col = np.sum((x[:, col] < range_[0]) | (x[:, col] > range_[1]))\n",
    "        removed_count += removed_count_col\n",
    "        \n",
    "        # Keep only values within the range\n",
    "        x[np.logical_or(x[:, col] < range_[0], x[:, col] > range_[1])] = 0\n",
    "        \n",
    "    return x, removed_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "## 2. Replace the missing values with the mean of the column, add columns \n",
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    return np.nan_to_num((x - np.nanmean(x, axis=0)) / np.nanstd(x, axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Data Cleaning Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    x_num, x_cat = split_num_cat(data)\n",
    "    x_num = standardize(x_num)\n",
    "    x_num, removed_cnt = remove_outliers(x_num)\n",
    "    x_num = np.c_[np.ones(len(x_num)), x_num]  # add the column of ones\n",
    "    x_cat = one_hot_encode(x_cat)\n",
    "    return np.hstack((x_num, x_cat))\n",
    "\n",
    "\n",
    "x_train_std = process_data(x_train)\n",
    "x_test_std = process_data(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_num = standardize(x_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = np.apply_along_axis(max, 0, x_num)\n",
    "mn = np.apply_along_axis(min, 0, x_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, rcnt = remove_outliers(x_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_outliers(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yiyuan's test code -- not sure how is this correlation numpy function working\n",
    "# temp = np.array([[1, 2, 7], [19, 58, 37], [-2, 6, 14]])\n",
    "# tempy = np.transpose(np.array([0, 3, -10]))\n",
    "# print(temp)\n",
    "# print(\"y\", tempy)\n",
    "\n",
    "# print(\"correlation is \\n\", np.corrcoef(temp, tempy, rowvar=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_correlation(x_train, y_train):\n",
    "    return np.abs(np.corrcoef(x_train, y_train, rowvar=False))\n",
    "\n",
    "cr  =  feature_correlation(x_train_std, y_train)[-1, :-1]\n",
    "plt.hist(cr, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Screen out features based on correlation\n",
    "good_corre_indicies = np.argwhere(cr > 0.05).flatten()\n",
    "print(\"good_corre_indicies\", good_corre_indicies)\n",
    "x_train_corre = x_train_std[:, good_corre_indicies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression *without* regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "initial_w = np.zeros(x_train_corre.shape[1], dtype=np.float128)\n",
    "max_iters = 100\n",
    "gamma = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "## Here the logistic regression is from implementations.py\n",
    "## \n",
    "w, loss = logistic_regression(y_train, x_train_corre, initial_w, max_iters, gamma)\n",
    "print(\"loss is \", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to predict x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def prediction_labels(weights, data):  ## isn't this for linear regression only ? Don't we need the sigmoid?\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix.\"\"\"\n",
    "    y_pred = sigmoid(np.dot(data, weights))\n",
    "    y_pred[np.where(y_pred >= 0.5)] = 1\n",
    "    y_pred[np.where(y_pred < 0.5)] = 0\n",
    "    return y_pred\n",
    "\n",
    "y_pred = prediction_labels(w, x_train_corre)\n",
    "temp = y_pred[y_pred != 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_train):\n",
    "    return (y_pred == y_train).sum() / len(y_train)\n",
    "def precision(y_pred, y_train):\n",
    "    TP = np.sum((y_train==1) & (y_pred==1))\n",
    "    FP = np.sum((y_train==0) & (y_pred==1))\n",
    "    return TP/(TP+FP)\n",
    "def recall(y_pred, y_train):\n",
    "    recall = np.sum((y_train==1) & (y_pred==1)) / np.sum(y_train==1)\n",
    "    return recall\n",
    "def f_score (y_pred, y_train):\n",
    "    return 2*precision(y_pred, y_train)*recall(y_pred, y_train) / (precision(y_pred, y_train) + recall(y_pred, y_train))\n",
    "\n",
    "print(\"accuracy\", accuracy(y_pred, y_train))\n",
    "print(\"precision\", precision(y_pred, y_train))\n",
    "print(\"recall\", recall(y_pred, y_train))\n",
    "print(\"f_score\", f_score(y_pred, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate trained data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "y_pred = prediction_labels(w, x_test_std[:, good_corre_indicies])\n",
    "y_pred[y_pred == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(test_ids, y_pred, 'submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
